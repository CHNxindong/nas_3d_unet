{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import nibabel as nib\n",
    "import os\n",
    "import glob\n",
    "from dev_tools.my_tools import print_red, minmax_normalize\n",
    "import pdb\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "def create_h5(source_folder, mean_std_file, overwrite=False):\n",
    "    try:\n",
    "        affine = np.load('data/affine.npy')\n",
    "    except FileNotFoundError:\n",
    "        affine = None\n",
    "    \n",
    "    target = os.path.join('data',source_folder.split('_')[-1]+'.h5')\n",
    "    \n",
    "    if os.path.exists(target) and not overwrite:\n",
    "        print('{:s} exists already.'.format(target))\n",
    "        return\n",
    "    \n",
    "    with open(mean_std_file,'rb') as f:\n",
    "        mean_std_values = pickle.load(f)\n",
    "    \n",
    "    with h5py.File(target,'w') as f:\n",
    "        img_dirs  = glob.glob(os.path.join(source_folder,'*/*' \n",
    "                                             if source_folder.split('_')[-1] == 'Training' else '*'))\n",
    "        for img_dir in tqdm(img_dirs,desc='writing {:s}'.format(target)):\n",
    "            if not os.path.isdir(img_dir):\n",
    "                continue\n",
    "            sub_id = img_dir.split('/')[-1]\n",
    "            h5_subid = f.create_group(sub_id)\n",
    "            brain_widths = []\n",
    "            for mod_file in os.listdir(img_dir):\n",
    "                img = nib.load(os.path.join(img_dir,mod_file))\n",
    "                if affine is None:\n",
    "                    affine = img.affine\n",
    "                    np.save('data/affine',affine)\n",
    "                img_npy = img.get_data()\n",
    "                mod = mod_file.split('_')[-1].split('.')[0]\n",
    "                if mod != 'seg':\n",
    "                    img_npy = normalize(img_npy,\n",
    "                                        mean = mean_std_values['{:s}_mean'.format(mod)],\n",
    "                                        std = mean_std_values['{:s}_std'.format(mod)])\n",
    "                    brain_widths.append(cal_outline(img_npy))\n",
    "                h5_subid.create_dataset(mod_file,data=img_npy)\n",
    "            start_edge = np.min(brain_widths,axis=0)[0]\n",
    "            end_edge = np.max(brain_widths,axis=0)[1]\n",
    "            brain_width = np.vstack((start_edge,end_edge))\n",
    "            h5_subid.create_dataset('brain_width',data=brain_width)\n",
    "    return\n",
    "\n",
    "def cal_outline(img_npy):\n",
    "    '''\n",
    "    return an numpy array shape=(2,3), indicating the outline of the brain area.\n",
    "    '''\n",
    "    brain_index = np.asarray(np.nonzero(img_npy))\n",
    "    start_edge = np.maximum(np.min(brain_index,axis=1)-1,0)\n",
    "    end_edge = np.minimum(np.max(brain_index,axis=1)+1,img_npy.shape)\n",
    "    \n",
    "    return np.vstack((start_edge,end_edge))\n",
    "\n",
    "def normalize(img_npy,mean,std,offset=0.1, mul_factor=100):\n",
    "    '''\n",
    "    offset and mul_factor are used to make a distinction between brain voxel and background(zeros).\n",
    "    '''\n",
    "    brain_index = np.nonzero(img_npy)\n",
    "    img_npy[brain_index] = (minmax_normalize((img_npy[brain_index]-mean)/std) + offset) * mul_factor\n",
    "    return img_npy\n",
    "\n",
    "\n",
    "def cal_mean_std(source_folder,saved_path,overwrite=False):\n",
    "    '''\n",
    "    Calculte the mean value and standard deviation for each modalities.\n",
    "    Return a dictionary {'t1_mean': ,'t1_std': ,'t2_mean': ,'t2_std': ,...}\n",
    "    '''\n",
    "    if os.path.exists(saved_path) and not overwrite:\n",
    "        print('{:s} exists already.'.format(saved_path))\n",
    "        return\n",
    "    sub_dirs = glob.glob(os.path.join(source_folder,'*/*')) # SD\n",
    "    \n",
    "    mean_std_values = {}\n",
    "    \n",
    "    for mod in config['data']['all_mods']:\n",
    "        mean = 0\n",
    "        amount = 0\n",
    "        for sub_dir in tqdm(sub_dirs,\n",
    "                             desc='Calculating {:s}\\'s mean value'\n",
    "                             .format(mod)):\n",
    "            file_name = os.path.join(sub_dir,sub_dir.split('/')[-1]+'_{:s}.nii.gz'.format(mod))\n",
    "            img_npy = nib.load(file_name).get_data()\n",
    "            brain_area = img_npy[np.nonzero(img_npy)]\n",
    "            mean += np.sum(brain_area)\n",
    "            amount += len(brain_area)\n",
    "        mean /= amount\n",
    "        mean_std_values['{:s}_mean'.format(mod)] = round(mean,4)\n",
    "        print('{:s}\\'s mean value = {:.2f}'.format(mod,mean))\n",
    "        \n",
    "        std = 0\n",
    "        for sub_dir in tqdm(sub_dirs,\n",
    "                             desc='Calculating {:s}\\'s std value'\n",
    "                             .format(mod)):\n",
    "            file_name = os.path.join(sub_dir,sub_dir.split('/')[-1]+'_{:s}.nii.gz'.format(mod))\n",
    "            img_npy = nib.load(file_name).get_data()\n",
    "            brain_area = img_npy[np.nonzero(img_npy)]\n",
    "            std += np.sum((brain_area-mean)**2)\n",
    "        std = np.sqrt(std/amount)\n",
    "        mean_std_values['{:s}_std'.format(mod)] = round(std,4)\n",
    "        print('{:s}\\'s std value = {:.2f}'.format(mod,std))\n",
    "    print(mean_std_values)\n",
    "    with open(saved_path,'wb') as f:\n",
    "        pickle.dump(mean_std_values,f)\n",
    "   \n",
    "                          \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_test():\n",
    "    with open('config.yml') as f:\n",
    "        config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "    cal_mean_std(source_folder=config['data']['source_train'],\n",
    "                 saved_path=config['data']['mean_std_file'])\n",
    "\n",
    "    mean_std_file = config['data']['mean_std_file']\n",
    "    create_h5(config['data']['source_train'],mean_std_file)\n",
    "    create_h5(config['data']['source_val'],mean_std_file)\n",
    "    create_h5(config['data']['source_test'],mean_std_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990000000000001"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/3,3) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cross_val_indices.pkl exists already.\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "    \n",
    "def cross_val_split(num_sbjs, saved_path, num_folds=5, overwrite=False):\n",
    "    '''\n",
    "    To generate num_folds cross validation.\n",
    "    Return {'subid index list'}\n",
    "    '''\n",
    "    if os.path.exists(saved_path) and not overwrite:\n",
    "        print('{:s} exists already.'.format(saved_path))\n",
    "        return\n",
    "    subid_indices = list(range(num_sbjs))\n",
    "    shuffle(subid_indices)\n",
    "    res = {}\n",
    "    for i in range(num_folds):\n",
    "        left = int(i/num_folds * num_sbjs)\n",
    "        right = int((i+1)/num_folds * num_sbjs)\n",
    "        res['train_list_{:d}'.format(i)] = subid_indices[:left] + subid_indices[right:]\n",
    "        res['val_list_{:d}'.format(i)] = subid_indices[left : right]\n",
    "    with open(saved_path,'wb') as f:\n",
    "        pickle.dump(res,f)\n",
    "        \n",
    "        \n",
    "\n",
    "with open('config.yml') as f:\n",
    "    config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "with h5py.File('data/Training.h5','r') as f:\n",
    "    num_sbjs = len(f)\n",
    "cross_val_indices = config['data']['cross_val_indices']  \n",
    "cross_val_split(num_sbjs, cross_val_indices)\n",
    "\n",
    "def get_training_and_validation_generators(data_file, batch_size, n_labels, training_keys_file, \n",
    "                                           validation_keys_file,\n",
    "                                           data_split=0.8, overwrite=False, labels=None, augment=False,\n",
    "                                           augment_flip=True, augment_distortion_factor=0.25, \n",
    "                                           patch_shape=None,\n",
    "                                           validation_patch_overlap=0, training_patch_start_offset=None,\n",
    "                                           validation_batch_size=None, skip_blank=True, permute=False,\n",
    "                                           num_model=1,\n",
    "                                           pred_specific=False, overlap_label=True,\n",
    "                                           for_final_val=False):\n",
    "    pass\n",
    "    #     pdb.set_trace()\n",
    "#     if not validation_batch_size:\n",
    "#         validation_batch_size = batch_size\n",
    "\n",
    "#     training_list, validation_list = get_validation_split(data_file,\n",
    "#                                                           data_split=data_split,\n",
    "#                                                           overwrite=overwrite,\n",
    "#                                                           training_file=training_keys_file,\n",
    "#                                                           validation_file=validation_keys_file)\n",
    "#     if for_final_val:\n",
    "#         training_list = training_list + validation_list\n",
    "\n",
    "#     training_generator = data_generator(data_file, training_list,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         n_labels=n_labels,\n",
    "#                                         labels=labels,\n",
    "#                                         augment=augment,\n",
    "#                                         augment_flip=augment_flip,\n",
    "#                                         augment_distortion_factor=augment_distortion_factor,\n",
    "#                                         patch_shape=patch_shape,\n",
    "#                                         patch_overlap=validation_patch_overlap,\n",
    "#                                         patch_start_offset=training_patch_start_offset,\n",
    "#                                         skip_blank=skip_blank,\n",
    "#                                         permute=permute,\n",
    "#                                         num_model=num_model,\n",
    "#                                         pred_specific=pred_specific,\n",
    "#                                         overlap_label=overlap_label)\n",
    "\n",
    "#     validation_generator = data_generator(data_file, validation_list,\n",
    "#                                           batch_size=validation_batch_size,\n",
    "#                                           n_labels=n_labels,\n",
    "#                                           labels=labels,\n",
    "#                                           patch_shape=patch_shape,\n",
    "#                                           patch_overlap=validation_patch_overlap,\n",
    "#                                           skip_blank=skip_blank,\n",
    "#                                           num_model=num_model,\n",
    "#                                           pred_specific=pred_specific,\n",
    "#                                           overlap_label=overlap_label)\n",
    "\n",
    "#     # Set the number of training and testing samples per epoch correctly\n",
    "#     #     pdb.set_trace()\n",
    "#     if os.path.exists('num_patches_training.npy'):\n",
    "#         num_patches_training = int(np.load('num_patches_training.npy'))\n",
    "#     else:\n",
    "#         num_patches_training = get_number_of_patches(data_file, training_list, patch_shape,\n",
    "#                                                        skip_blank=skip_blank,\n",
    "#                                                        patch_start_offset=training_patch_start_offset,\n",
    "#                                                        patch_overlap=validation_patch_overlap,\n",
    "#                                                        pred_specific=pred_specific)\n",
    "#         np.save('num_patches_training', num_patches_training)\n",
    "#     num_training_steps = get_number_of_steps(num_patches_training, batch_size)\n",
    "#     print(\"Number of training steps in each epoch: \", num_training_steps)\n",
    "\n",
    "#     if os.path.exists('num_patches_val.npy'):\n",
    "#         num_patches_val = int(np.load('num_patches_val.npy'))\n",
    "#     else:\n",
    "#         num_patches_val = get_number_of_patches(data_file, validation_list, patch_shape,\n",
    "#                                                  skip_blank=skip_blank,\n",
    "#                                                  patch_overlap=validation_patch_overlap,\n",
    "#                                                  pred_specific=pred_specific)\n",
    "#         np.save('num_patches_val', num_patches_val)\n",
    "#     num_validation_steps = get_number_of_steps(num_patches_val, validation_batch_size)\n",
    "#     print(\"Number of validation steps in each epoch: \", num_validation_steps)\n",
    "\n",
    "#     return training_generator, validation_generator, num_training_steps, num_validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .utils import pickle_dump, pickle_load\n",
    "from .patches import compute_patch_indices, get_random_nd_index, get_patch_from_3d_data, compute_patch_indices_for_prediction\n",
    "from .augment import augment_data, random_permutation_x_y\n",
    "\n",
    "import pdb\n",
    "from dev_tools.my_tools import print_red\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class train_generator(Generator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self,config_file):\n",
    "        with open(config_file) as f:\n",
    "            config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# train_generator, validation_generator, n_train_steps, n_validation_steps = get_training_and_validation_generators(\n",
    "#         data_file_opened,\n",
    "#         batch_size=config[\"batch_size\"],\n",
    "#         data_split=config[\"validation_split\"],\n",
    "#         overwrite=overwrite,\n",
    "#         validation_keys_file=config[\"validation_file\"],\n",
    "#         training_keys_file=config[\"training_file\"],\n",
    "#         n_labels=config[\"n_labels\"],\n",
    "#         labels=config[\"labels\"],\n",
    "#         patch_shape=config[\"patch_shape\"],\n",
    "#         validation_batch_size=config[\"validation_batch_size\"],\n",
    "#         validation_patch_overlap=config[\"validation_patch_overlap\"],\n",
    "#         training_patch_start_offset=config[\"training_patch_start_offset\"],\n",
    "#         permute=config[\"permute\"],\n",
    "#         augment=config[\"augment\"],\n",
    "#         skip_blank=config[\"skip_blank\"],\n",
    "#         augment_flip=config[\"flip\"],\n",
    "#         augment_distortion_factor=config[\"distort\"],\n",
    "#         pred_specific=config['pred_specific'],\n",
    "#         overlap_label=config['overlap_label_generator'],\n",
    "#         for_final_val=config['for_final_val'])        \n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    def get_number_of_steps(n_samples, batch_size):\n",
    "        if n_samples <= batch_size:\n",
    "            return n_samples\n",
    "        elif np.remainder(n_samples, batch_size) == 0:\n",
    "            return n_samples//batch_size\n",
    "        else:\n",
    "            return n_samples//batch_size + 1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def data_generator(data_file, index_list, batch_size=1, n_labels=1, labels=None, augment=False, augment_flip=True,\n",
    "                       augment_distortion_factor=0.25, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                       shuffle_index_list=True, skip_blank=True, permute=False, num_model=1, pred_specific=False,overlap_label=False):\n",
    "    #     pdb.set_trace()\n",
    "\n",
    "        orig_index_list = index_list\n",
    "        while True:\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "            if patch_shape:\n",
    "                index_list = create_patch_index_list(orig_index_list, data_file, patch_shape,\n",
    "                                                     patch_overlap, patch_start_offset,pred_specific=pred_specific)\n",
    "            else:\n",
    "                index_list = copy.copy(orig_index_list)\n",
    "\n",
    "            if shuffle_index_list:\n",
    "                shuffle(index_list)\n",
    "            while len(index_list) > 0:\n",
    "                index = index_list.pop()\n",
    "                add_data(x_list, y_list, data_file, index, augment=augment, augment_flip=augment_flip,\n",
    "                         augment_distortion_factor=augment_distortion_factor, patch_shape=patch_shape,\n",
    "                         skip_blank=skip_blank, permute=permute)\n",
    "                if len(x_list) == batch_size or (len(index_list) == 0 and len(x_list) > 0):\n",
    "                    yield convert_data(x_list, y_list, n_labels=n_labels, labels=labels, num_model=num_model,overlap_label=overlap_label)\n",
    "    #                 convert_data(x_list, y_list, n_labels=n_labels, labels=labels, num_model=num_model)\n",
    "                    x_list = list()\n",
    "                    y_list = list()\n",
    "\n",
    "\n",
    "\n",
    "    def get_number_of_patches(data_file, index_list, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                              skip_blank=True,pred_specific=False):\n",
    "        if patch_shape:\n",
    "            index_list = create_patch_index_list(index_list, data_file, patch_shape, patch_overlap,\n",
    "                                                 patch_start_offset,pred_specific=pred_specific)\n",
    "            count = 0\n",
    "            for index in tqdm(index_list):\n",
    "                x_list = list()\n",
    "                y_list = list()\n",
    "                add_data(x_list, y_list, data_file, index, skip_blank=skip_blank, patch_shape=patch_shape)\n",
    "                if len(x_list) > 0:\n",
    "                    count += 1\n",
    "            return count\n",
    "        else:\n",
    "            return len(index_list)\n",
    "\n",
    "\n",
    "    def create_patch_index_list(index_list, data_file, patch_shape, patch_overlap, patch_start_offset=None, pred_specific=False):\n",
    "        patch_index = list()\n",
    "        for index in index_list:\n",
    "            brain_width = data_file.root.brain_width[index]\n",
    "            image_shape = brain_width[1] - brain_width[0] + 1\n",
    "            if pred_specific:\n",
    "                patches = compute_patch_indices_for_prediction(image_shape, patch_shape)\n",
    "            else:\n",
    "                if patch_start_offset is not None:\n",
    "                    random_start_offset = np.negative(get_random_nd_index(patch_start_offset))\n",
    "                    patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap, start=random_start_offset)\n",
    "                else:\n",
    "                    patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap)\n",
    "            patch_index.extend(itertools.product([index], patches))\n",
    "        return patch_index\n",
    "\n",
    "\n",
    "    def add_data(x_list, y_list, data_file, index, augment=False, augment_flip=False, augment_distortion_factor=0.25,\n",
    "                 patch_shape=False, skip_blank=True, permute=False):\n",
    "        '''\n",
    "        add qualified x,y to the generator list\n",
    "        '''\n",
    "    #     pdb.set_trace()\n",
    "        data, truth = get_data_from_file(data_file, index, patch_shape=patch_shape)\n",
    "\n",
    "        if np.sum(truth) == 0:\n",
    "            return\n",
    "        if augment:\n",
    "            affine = np.load('affine.npy')\n",
    "            data, truth = augment_data(data, truth, affine, flip=augment_flip, scale_deviation=augment_distortion_factor)\n",
    "\n",
    "        if permute:\n",
    "            if data.shape[-3] != data.shape[-2] or data.shape[-2] != data.shape[-1]:\n",
    "                raise ValueError(\"To utilize permutations, data array must be in 3D cube shape with all dimensions having \"\n",
    "                                 \"the same length.\")\n",
    "            data, truth = random_permutation_x_y(data, truth[np.newaxis])\n",
    "        else:\n",
    "            truth = truth[np.newaxis]\n",
    "\n",
    "        if not skip_blank or np.any(truth != 0):\n",
    "            x_list.append(data)\n",
    "            y_list.append(truth)\n",
    "\n",
    "\n",
    "    def get_data_from_file(data_file, index, patch_shape=None):\n",
    "    #     pdb.set_trace()\n",
    "        if patch_shape:\n",
    "            index, patch_index = index\n",
    "            data, truth = get_data_from_file(data_file, index, patch_shape=None)\n",
    "            x = get_patch_from_3d_data(data, patch_shape, patch_index)\n",
    "            y = get_patch_from_3d_data(truth, patch_shape, patch_index)\n",
    "        else:\n",
    "            brain_width = data_file.root.brain_width[index]\n",
    "            x = np.array([modality_img[index,0,\n",
    "                                       brain_width[0,0]:brain_width[1,0]+1,\n",
    "                                       brain_width[0,1]:brain_width[1,1]+1,\n",
    "                                       brain_width[0,2]:brain_width[1,2]+1] \n",
    "                          for modality_img in [data_file.root.t1,\n",
    "                                               data_file.root.t1ce,\n",
    "                                               data_file.root.flair,\n",
    "                                               data_file.root.t2]])\n",
    "            y = data_file.root.truth[index, 0,\n",
    "                                     brain_width[0,0]:brain_width[1,0]+1,\n",
    "                                     brain_width[0,1]:brain_width[1,1]+1,\n",
    "                                     brain_width[0,2]:brain_width[1,2]+1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def convert_data(x_list, y_list, n_labels=1, labels=None, num_model=1,overlap_label=False):\n",
    "    #     pdb.set_trace()\n",
    "        x = np.asarray(x_list)\n",
    "        y = np.asarray(y_list)\n",
    "        if n_labels == 1:\n",
    "            y[y > 0] = 1\n",
    "        elif n_labels > 1:\n",
    "            if overlap_label:\n",
    "                y = get_multi_class_labels_overlap(y, n_labels=n_labels, labels=labels)\n",
    "            else:\n",
    "                y = get_multi_class_labels(y, n_labels=n_labels, labels=labels)\n",
    "        if num_model == 1:\n",
    "            return x, y\n",
    "        else:\n",
    "            return [x]*num_model, y\n",
    "\n",
    "\n",
    "    def get_multi_class_labels_overlap(data, n_labels=3, labels=(1,2,4)):\n",
    "        \"\"\"\n",
    "        4: ET\n",
    "        1+4: TC\n",
    "        1+2+4: WT\n",
    "        \"\"\"\n",
    "    #     pdb.set_trace()\n",
    "        new_shape = [data.shape[0], n_labels] + list(data.shape[2:])\n",
    "        y = np.zeros(new_shape, np.int8)\n",
    "\n",
    "        y[:,0][np.logical_or(data[:,0] == 1,data[:,0] == 4)] = 1    #1\n",
    "        y[:,1][np.logical_or(data[:,0] == 1,data[:,0] == 2, data[:,0] == 4)] = 1 #2\n",
    "        y[:,2][data[:,0] == 4] = 1    #4\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dev_tools.my_tools import print2d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = nib.load('data/MICCAI_BraTS_2019_Data_Training/HGG\\\n",
    "/BraTS19_TMC_11964_1/BraTS19_TMC_11964_1_t1.nii.gz').get_data()\n",
    "plt.figure()\n",
    "plt.hist(np.ravel(img))\n",
    "print2d(img)\n",
    "with h5py.File('data/Training.h5','r') as f:\n",
    "    norm_img = f['BraTS19_TMC_11964_1']['BraTS19_TMC_11964_1_t1.nii.gz']\n",
    "    print2d(norm_img)\n",
    "    plt.figure()\n",
    "    plt.hist(np.ravel(norm_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534.3123637025672"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = img[np.nonzero(img)]\n",
    "np.sum(b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1,'b':2}\n",
    "with open('test.pkl','wb') as f:\n",
    "    pickle.dump(a,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t1_std': 1082.5379, 't1_mean': 571.9798, 't1ce_std': 1093.0112, 't2_mean': 652.5108, 'flair_mean': 411.4047, 't2_std': 1285.4105, 'flair_std': 1219.138, 't1ce_mean': 637.505}\n"
     ]
    }
   ],
   "source": [
    "with open('data/mean_std.pkl','rb') as f:\n",
    "    res = pickle.load(f)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1,2,3,50,34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.916666666666664"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean([1,34]),np.mean([2,3,50])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
