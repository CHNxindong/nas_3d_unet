{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from search import Searching\n",
    "from train import Training\n",
    "from prediction import Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/spe.pkl','rb') as f:\n",
    "    print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# s = Searching()\n",
    "# gene = s.search()\n",
    "\n",
    "# t = Training()\n",
    "# t.main_run()\n",
    "\n",
    "t = Training(for_final_training=True, new_lr=True)\n",
    "t.main_run()\n",
    "\n",
    "# p = Prediction()\n",
    "# p.predict()\n",
    "# p.predict(p.config['data']['training_h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sd = torch.load('log/last_train.pt')\n",
    "his = sd['history']\n",
    "loss = his['loss']\n",
    "val_loss = his['val_loss']\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(range(len(loss)), loss, label='loss')\n",
    "plt.plot(range(len(loss)), val_loss, label='val_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Weighted Dice Loss')\n",
    "plt.legend()\n",
    "plt.savefig('log/training_log.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load('log/last_search.pt')\n",
    "his = sd['history']\n",
    "shell_loss = his['shell_loss']\n",
    "kernel_loss = his['kernel_loss']\n",
    "val_loss = his['val_loss']\n",
    "n = range(len(shell_loss))\n",
    "plt.plot(n, shell_loss, label='shell_loss')\n",
    "plt.plot(n, kernel_loss, label='kernel_loss')\n",
    "plt.plot(n, val_loss, label='val_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Weighted Dice Loss')\n",
    "plt.legend()\n",
    "plt.savefig('log/searching_log.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from collections import namedtuple\n",
    "Genotype = namedtuple('Genotype', ['down','down_concat','up','up_concat'])\n",
    "\n",
    "\n",
    "with open('BACKUP/best_genotype.pkl','rb') as f:\n",
    "    t = pickle.load(f)\n",
    "    g = eval(t[0])\n",
    "    n_g = t[1]\n",
    "from genotype import Genotype\n",
    "new_g = Genotype(down=g.down, up=g.up)\n",
    "with open('BACKUP/new_best_genotype.pkl','wb') as f:\n",
    "    pickle.dump((str(new_g), n_g),f)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from genotype import Genotype\n",
    "from plot import plot_cell,plot_searched_cell,plot_ops\n",
    "\n",
    "plot_cell('log/dc')\n",
    "plot_cell('log/uc', dc=False)\n",
    "\n",
    "with open('BACKUP/new_best_genotype.pkl','rb') as f:\n",
    "    g = eval(pickle.load(f)[0])\n",
    "plot_searched_cell(g.down, 'log/searched_dc')\n",
    "plot_searched_cell(g.up, 'log/searched_uc', dc=False)\n",
    "\n",
    "plot_ops('log/ops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import draw_evaluate, four_in_all\n",
    "draw_evaluate('data/results/Stats_Training_final.csv','log/training_figs', val=False)\n",
    "draw_evaluate('data/results/Stats_Validation_final.csv','log/val_figs')\n",
    "four_in_all('log/training_figs')\n",
    "four_in_all('log/val_figs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in [(i,j) for i in [0,1] for j in [0,1]]:\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen():\n",
    "    for i in range(20):\n",
    "        yield i\n",
    "g = gen()\n",
    "st = time.time()\n",
    "desc = 'epoch {}'.format(0)\n",
    "with tqdm(g,total=20,desc=desc) as t:\n",
    "    for step, x in enumerate(t):\n",
    "        time.sleep(0.1)\n",
    "        info = collections.OrderedDict()\n",
    "        info['AShellLoss'] = '{:03d}'.format(step)\n",
    "        info['KernelLoss'] = x\n",
    "        t.set_postfix(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = collections.OrderedDict()\n",
    "d['K'] = 1\n",
    "d['A'] = 2\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    " \n",
    "formatter = logging.Formatter('%(message)s')\n",
    "# Configure stream handler for the cells\n",
    "chandler = logging.StreamHandler()\n",
    "# chandler.setLevel(logging.INFO)\n",
    "chandler.setFormatter(formatter)\n",
    " \n",
    "# Add both handlers\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    " \n",
    "# Show the handlers\n",
    "# logger.handlers\n",
    " \n",
    "# Log Something\n",
    "logger.info(\"Test info\")\n",
    "logger.debug(\"Test debug\")\n",
    "logger.error(\"Test error\")\n",
    "while logger.handlers:\n",
    "    logger.handlers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import generator\n",
    "from dev_tools.my_tools import print2d\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "def test():\n",
    "#     pdb.set_trace()\n",
    "    temp = time.time()\n",
    "    dataset = generator.Dataset()\n",
    "    train = dataset.train_generator\n",
    "    val = dataset.val_generator\n",
    "    epoch = val.epoch()\n",
    "    for i,(x,y) in tqdm(enumerate(epoch),total = val.steps_per_epoch):\n",
    "#         pass\n",
    "        return torch.as_tensor(x,device=torch.device('cuda')),torch.as_tensor(y,device=torch.device('cuda'))\n",
    "        \n",
    "    print(time.time() - temp)\n",
    "x,y = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "a = torch.Tensor(x)\n",
    "b = torch.from_numpy(x)\n",
    "b[0,0] = 1000\n",
    "c = torch.as_tensor(x,device=device)\n",
    "# b = b.cuda()\n",
    "# print(a.dtype,b.dtype)\n",
    "# c.device\n",
    "# b.device\n",
    "# b.cuda()\n",
    "# c = torch.Tensor(2,3)\n",
    "# c.cuda()\n",
    "print(a,b,c)\n",
    "a = a.cuda()\n",
    "print(a)\n",
    "b = b.to(device)\n",
    "print(b)\n",
    "b[0,0] = 100\n",
    "c[0,0] = 100\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.random.randint(2, size = (2,3,128,128,128))\n",
    "# a = tf.constant(x,dtype = float)\n",
    "# with tf.Session() as s:\n",
    "#     print(dice_coefficient(a,a).eval())\n",
    "axis=(-1,-2,-3)\n",
    "smooth=0.00001\n",
    "#     temp = (2*K.sum(a * a, axis=axis) + smooth)/(K.sum(a, axis=axis) + K.sum(a,axis=axis) + smooth)\n",
    "#     print(temp.eval())\n",
    "#     print(K.mean(temp).eval())\n",
    "x = torch.as_tensor(a, device=torch.device('cuda'))\n",
    "xx = torch.as_tensor(a, device = torch.device('cpu'))\n",
    "import loss\n",
    "w_dice = loss.WeightedDiceLoss()\n",
    "w_dice(x, y).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml') as f:\n",
    "    config = yaml.load(f,Loader=yaml.FullLoader)\n",
    "    print(config['data'])\n",
    "with open('a.yml','w') as f:\n",
    "    yaml.dump({'a':[1,2,3]},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "class test():\n",
    "    def __init__(self):\n",
    "        self.h5_f = h5py.File('data/training.h5','r')\n",
    "    def hello(self):\n",
    "        print(len(list(self.h5_f.keys())))\n",
    "    def end(self):\n",
    "        self.h5_f.close()\n",
    "t = test()\n",
    "t.hello()\n",
    "t.end()\n",
    "t.__init__()\n",
    "t.hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# temp = time.time()\n",
    "# print(patching_autofit((240,240,155),(128,128,128)))\n",
    "# print(time.time()-temp)\n",
    "# temp = time.time()\n",
    "# print(patching_hardcode128((240,240,155),(128,128,128)))\n",
    "# print(time.time()-temp)\n",
    "# print(patching((240,240,155),(128,128,128)))\n",
    "\n",
    "# print(np.vstack((a,b)))\n",
    "print('\\n')\n",
    "print(patching((240,240,155),(128,128,128)))\n",
    "print('\\n')\n",
    "print(patching((240,240,155),(128,128,128),overlap=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float('inf') == float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([8,6+12]) -18) % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1,'b':2}\n",
    "with open('test.pkl','wb') as f:\n",
    "    pickle.dump(a,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mean_std.pkl','rb') as f:\n",
    "    res = pickle.load(f)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([1,2,3,50,34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.mean([1,34]),np.mean([2,3,50])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
